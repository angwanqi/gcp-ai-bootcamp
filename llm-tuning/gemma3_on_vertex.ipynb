{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed6b1b-3252-45a6-a4c9-ebc513d6cd23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebfcf97-7aa7-41eb-98d1-e51768d56ee8",
   "metadata": {},
   "source": [
    "# Gemma 3 on Vertex AI\n",
    "\n",
    "Adapted from: [Vertex AI Model Garden - Gemma 3 Deployment on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma3_deployment_on_vertex.ipynb)\n",
    "\n",
    "Modified by: [Wan Qi Ang](https://github.com/angwanqi)\n",
    "\n",
    "Last updated: 21 May 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17715826-f5eb-4677-a0d7-0c51d1de6cde",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6818a99b-1b33-49b2-afaa-5c6f897b937e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8afd31e-3540-427e-86de-ef7a57b94a74",
   "metadata": {},
   "source": [
    "### Set your project ID and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1e77b-cac2-4760-b405-2be8c4eb706f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the default cloud project id.\n",
    "PROJECT_ID= !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "# Set the default region\n",
    "REGION = \"asia-southeast1\"\n",
    "\n",
    "use_dedicated_endpoint=False\n",
    "\n",
    "endpoints = {}\n",
    "\n",
    "print(f\"Project ID:\", PROJECT_ID)\n",
    "print(f\"Project Region:\", REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb9fa6-fce4-4c03-ae2d-61ae68e74f67",
   "metadata": {},
   "source": [
    "## Predicting with Vertex AI Prediction Endpoints\n",
    "\n",
    "### Load your existing model\n",
    "\n",
    "First, let's retrieved the details of the endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75e715-0b73-46eb-83a6-c08a9965b895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the Vertex AI Prediction Endpoint ID and set it\n",
    "all_endpoints = aiplatform.Endpoint.list(location=REGION)\n",
    "\n",
    "for endpoint in all_endpoints:\n",
    "    full_endpoint = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}\"\n",
    "    \n",
    "    if endpoint.display_name == \"ai-takeoff-gemma\":\n",
    "        endpoints['gemma3'] = aiplatform.Endpoint(full_endpoint)\n",
    "\n",
    "print(f\"Gemma 3 Endpoint Name: {endpoints['gemma3'].display_name}\")\n",
    "\n",
    "# List all existing endpoints\n",
    "# !gcloud ai endpoints list --project=$PROJECT_ID --region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc056835-e319-44a6-8a3b-e0808764df8d",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "This part assumes that you have a Vertex endpoint with Gemma 3 deployed. With the model deployed, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Human: What is a car?\n",
    "\n",
    "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "```\n",
    "\n",
    "Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbafe20-8a07-4ca7-8643-1931840a4681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert your prompt here\n",
    "prompt = \"What is a car?\"\n",
    "\n",
    "max_tokens = 100\n",
    "temperature = 1.0\n",
    "top_p = 1.0\n",
    "top_k = 1\n",
    "\n",
    "# Set `raw_response` to `True` to obtain the raw model output. \n",
    "# Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
    "raw_response = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16187727-e9f3-4174-a619-9d35e1784447",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a3799-173c-480e-b328-342d5ba73193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"raw_response\": raw_response,\n",
    "    },\n",
    "]\n",
    "response = endpoints['gemma3'].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86974c-e74c-43bc-b430-616ad7b8d084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db1daa-1c7a-4c93-baf7-7022c7ab2124",
   "metadata": {},
   "source": [
    "For more information on ```<start_of_turn>``` and ```<end_of_turn>```, refer to this documentation https://ai.google.dev/gemma/docs/core/prompt-structure"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
