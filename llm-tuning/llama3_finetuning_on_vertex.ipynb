{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Finetuning Llama 3 with Text on Vertex AI\n",
    "\n",
    "Adapted from: [Vertex AI Model Garden - Llama 3 Finetuning on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama3_finetuning.ipynb)\n",
    "\n",
    "Modified by: [Wan Qi Ang](https://github.com/angwanqi)\n",
    "\n",
    "Last updated: 16 April 2025\n",
    "\n",
    "<table><tbody><tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_llama3_finetuning.ipynb\">\n",
    "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama3_finetuning.ipynb\">\n",
    "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates finetuning and deploying Llama 3 models with Vertex AI. All of the examples in this notebook use parameter efficient finetuning methods [PEFT (LoRA)](https://github.com/huggingface/peft) to reduce training and storage costs. LoRA (Low-Rank Adaptation) is one approach of Parameter Efficient FineTuning (PEFT), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during finetuning. Read more about LoRA in the following publication: [Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685).\n",
    "\n",
    "After finetuning, we can deploy models on Vertex with GPU.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Finetune Llama 3 models with Vertex AI Custom Training Jobs.\n",
    "- Deploy finetuned Llama 3 models on Vertex AI Prediction.\n",
    "- Send prediction requests to your finetuned Llama 3 models.\n",
    "\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "### Install Python Packages for Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeSXuWbC7D_i"
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "# Upgrade Vertex AI SDK.\n",
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform>=1.64.0'\n",
    "! rm -rf vertex-ai-samples && git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart current runtime\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "<b>NOTE:</b> Only restart the current runtime if you installed libraries. If you did not install new libraries, you do not need to restart the kernel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import importlib\n",
    "import os\n",
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default cloud project id.\n",
    "PROJECT_ID= !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "# # Get the default cloud project id.\n",
    "# PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
    "\n",
    "# # Get the default region for launching jobs.\n",
    "# if not REGION:\n",
    "#     if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
    "#         raise ValueError(\n",
    "#             \"REGION must be set. See\"\n",
    "#             \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
    "#             \" available cloud locations.\"\n",
    "#         )\n",
    "#     REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default region for launching jobs.\n",
    "REGION = \"asia-southeast1\"\n",
    "\n",
    "models, endpoints = {}, {}\n",
    "# Dedicated endpoint not supported yet\n",
    "use_dedicated_endpoint = False\n",
    "\n",
    "print(f\"Project ID:\", PROJECT_ID)\n",
    "print(f\"Project Region:\", REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Necessary APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set (or create) the Google Cloud Storage bucket\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>INPUT REQUIRED:</b> Replace <YOUR_NAME> with your name so that you'll be able to identify your Google Cloud Storage bucket later on. <b>Example:</b> BUCKET_PREFIX = \"john\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the bucket prefix with your name\n",
    "PREFIX = \"<YOUR_NAME>\"\n",
    "\n",
    "# Concatenate to get the full bucket name\n",
    "BUCKET_NAME = PREFIX + \"-llama3-tuning\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "# EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"llama3\")\n",
    "\n",
    "\n",
    "print(f\"ROOT_BUCKET_URI:\", BUCKET_URI)\n",
    "print(f\"STAGING_BUCKET_URI:\", STAGING_BUCKET)\n",
    "# print(f\"EXPERIMENT_BUCKET_URI:\", EXPERIMENT_BUCKET)\n",
    "print(f\"MODEL_BUCKET_URI:\", MODEL_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Cloud Storage Bucket\n",
    "!gcloud storage buckets create $BUCKET_URI --location=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Compute Engine Service Account\n",
    "#### Retrieve the default Compute Engine Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the default SERVICE_ACCOUNT.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign Cloud Storage admin IAM role to the Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
    "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Llama 3 models\n",
    "\n",
    "For GPU based finetuning and serving, choose between accessing Llama 3 models on [Hugging Face](https://huggingface.co/)\n",
    "or Vertex AI as described below.\n",
    "\n",
    "If you already obtained access to Llama 3 models on [Hugging Face](https://huggingface.co/), you can load models from there.\n",
    "Alternatively, you can also load the original Llama 3 models for finetuning and serving from Vertex AI after accepting the agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOAD_MODEL_FROM = \"Hugging Face\"  # Options: [\"Hugging Face\", \"Google Cloud\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Access Llama 3 models on Hugging Face for GPU based finetuning and serving\n",
    "You must provide a Hugging Face User Access Token (read) to access the Llama 3 models. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below.\n",
    "\n",
    "*--- Or ---*\n",
    "### Option 2: Access Llama 3 models on Vertex AI for GPU based serving\n",
    "The original models from Meta are converted into the Hugging Face format for serving in Vertex AI.\n",
    "Accept the model agreement to access the models:\n",
    "1. Open the [Llama 3 model card](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3) from [Vertex AI Model Garden](https://cloud.google.com/model-garden).\n",
    "2. Review and accept the agreement in the pop-up window on the model card page. If you have previously accepted the model agreement, there will not be a pop-up window on the model card page and this step is not needed.\n",
    "3. After accepting the agreement of Llama 3, a `gs://` URI containing Llama 3 pretrained and finetuned models will be shared.\n",
    "4. Paste the URI in the `VERTEX_AI_MODEL_GARDEN_LLAMA3` field below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_TOKEN = \"\"\n",
    "VERTEX_AI_MODEL_GARDEN_LLAMA3 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LOAD_MODEL_FROM == \"Hugging Face\":\n",
    "    assert (HF_TOKEN), \"Provide a read HF_TOKEN to load models from Hugging Face, or select a different model source.\"\n",
    "    \n",
    "if LOAD_MODEL_FROM == \"Google Cloud\":\n",
    "    assert (\n",
    "        VERTEX_AI_MODEL_GARDEN_LLAMA3\n",
    "    ), \"Click the agreement of Llama 3 in Vertex AI Model Garden, and get the GCS path of Llama 3 model artifacts.\"\n",
    "    print(\n",
    "        \"Copying Llama 3 model artifacts from\",\n",
    "        VERTEX_AI_MODEL_GARDEN_LLAMA3,\n",
    "        \"to \",\n",
    "        MODEL_BUCKET,\n",
    "    )\n",
    "    HF_TOKEN = \"\"\n",
    "\n",
    "    ! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA3/* $MODEL_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb56d402e84a"
   },
   "source": [
    "## Finetune with HuggingFace PEFT and deploy with vLLM on GPUs\n",
    "This notebook uses [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset as an example.\n",
    "You can set `dataset_name` to any existing [Hugging Face dataset](https://huggingface.co/datasets) name, and set `instruct_column_in_dataset` to the name of the dataset column containing training data. The [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) has only one column `text`, and therefore we set `instruct_column_in_dataset` to `text` in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template name or gs:// URI to a custom template.\n",
    "template = \"openassistant-guanaco\"\n",
    "\n",
    "# Hugging Face dataset name or gs:// URI to a custom JSONL dataset.\n",
    "train_dataset_name = \"timdettmers/openassistant-guanaco\"  \n",
    "train_split_name = \"train\"  \n",
    "eval_dataset_name = \"timdettmers/openassistant-guanaco\"  \n",
    "eval_split_name = \"test\"  \n",
    "\n",
    "# Name of the dataset column containing training text input.\n",
    "instruct_column_in_dataset = \"text\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KwAW99YZHTdy"
   },
   "outputs": [],
   "source": [
    "# Load the data and have a look at what's in it\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "df = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "\n",
    "# Print the first 2 rows\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the first row of data\n",
    "df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune\n",
    "This section demonstrates how to finetune the Llama 3 text only model and merge the finetuned LoRA adapter with the base model on Vertex AI. It uses the Vertex AI SDK to create and run the custom training jobs.\n",
    "\n",
    "The training job takes approximately between 10 to 20 mins to set-up. Once done, the training job is expected to take around <time taken?> with the default configuration. To find the training time, throughput, and memory usage of your training job, you can go to the training logs and check the log line of the last training epoch.\n",
    "\n",
    "**Note**:\n",
    "1. We recommend setting `finetuning_precision_mode` to `4bit` because it enables using fewer hardware resources for finetuning.\n",
    "2. We recommend using NVIDIA_L4 for 8B models and NVIDIA_A100_80GB for 70B models.\n",
    "3. If `max_steps>0`, it will precedence over `epochs`. One can set a small `max_steps` value to quickly check the pipeline.\n",
    "4. With the default setting, training takes between 1.5 ~ 2 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the base model ID that you would like to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a model variant of Llama 3.\n",
    "# All options - [\"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-8B-Instruct\", \"meta-llama/Meta-Llama-3-70B\", \"meta-llama/Meta-Llama-3-70B-Instruct\"]\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accelerator to use - [\"NVIDIA_L4\", \"NVIDIA_A100_80GB\"]\n",
    "accelerator_type = \"NVIDIA_L4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivVGS9dHXPOz",
    "outputId": "5db26149-257f-4b84-ecbc-87b97f0017ce"
   },
   "outputs": [],
   "source": [
    "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:stable_20240909\"\n",
    "\n",
    "if LOAD_MODEL_FROM == \"Google Cloud\":\n",
    "    if MODEL_ID == \"meta-llama/Meta-Llama-3-8B\":\n",
    "        base_model_id = \"llama3-8b-hf\"\n",
    "    elif MODEL_ID == \"meta-llama/Meta-Llama-3-8B-Instruct\":\n",
    "        base_model_id = \"llama3-8b-chat-hf\"\n",
    "    elif MODEL_ID == \"meta-llama/Meta-Llama-3-70B\":\n",
    "        base_model_id = \"llama3-70b-hf\"\n",
    "    elif MODEL_ID == \"meta-llama/Meta-Llama-3-70B-Instruct\":\n",
    "        base_model_id = \"llama3-70b-chat-hf\"\n",
    "    else:\n",
    "        raise ValueError(f\"Undefined model ID: {MODEL_ID}.\")\n",
    "    base_model_id = os.path.join(MODEL_BUCKET, base_model_id)\n",
    "else:\n",
    "    base_model_id = MODEL_ID\n",
    "\n",
    "# Batch size for finetuning.\n",
    "per_device_train_batch_size = 1 \n",
    "gradient_accumulation_steps = 8 \n",
    "# Maximum sequence length.\n",
    "max_seq_length = 4096 \n",
    "# Setting a positive `max_steps` here will override `num_epochs`\n",
    "max_steps = -1 \n",
    "num_epochs = 1.0  # @param{type:\"number\"}\n",
    "# Precision mode for finetuning.\n",
    "finetuning_precision_mode = \"4bit\"  # @param [\"4bit\", \"8bit\", \"float16\"]\n",
    "# Learning rate.\n",
    "learning_rate = 5e-5  # @param{type:\"number\"}\n",
    "lr_scheduler_type = \"cosine\"  # @param{type:\"string\"}\n",
    "# LoRA parameters.\n",
    "lora_rank = 16 \n",
    "lora_alpha = 32 \n",
    "lora_dropout = 0.05  # @param{type:\"number\"}\n",
    "enable_gradient_checkpointing = True\n",
    "attn_implementation = \"flash_attention_2\"\n",
    "optimizer = \"paged_adamw_32bit\"\n",
    "warmup_ratio = \"0.01\"\n",
    "report_to = \"tensorboard\"\n",
    "save_steps = 10\n",
    "logging_steps = save_steps\n",
    "\n",
    "# Worker pool spec.\n",
    "machine_type = None\n",
    "if \"8b\" in MODEL_ID.lower():\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        accelerator_count = 4\n",
    "        machine_type = \"g2-standard-48\"\n",
    "    elif accelerator_type == \"NVIDIA_A100_80GB\":\n",
    "        accelerator_count = 1\n",
    "        machine_type = \"a2-ultragpu-1g\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Recommended machine settings not found for: {accelerator_type}. To use another accelerator, edit this code block to pass in an appropriate `machine_type`, `accelerator_type`, and `accelerator_count` to the deploy_model_vllm function by clicking `Show Code` and then modifying the code.\"\n",
    "        )\n",
    "elif \"70b\" in MODEL_ID.lower():\n",
    "    if accelerator_type == \"NVIDIA_A100_80GB\":\n",
    "        accelerator_count = 4\n",
    "        machine_type = \"a2-ultragpu-4g\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Recommended machine settings not found for: {accelerator_type}. To use another accelerator, edit this code block to pass in an appropriate `machine_type`, `accelerator_type`, and `accelerator_count` to the deploy_model_vllm function by clicking `Show Code` and then modifying the code.\"\n",
    "        )\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model ID or GCS path: {MODEL_ID}.\")\n",
    "\n",
    "replica_count = 1\n",
    "\n",
    "common_util.check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    is_for_training=True,\n",
    ")\n",
    "\n",
    "job_name = common_util.get_job_name_with_datetime(\"llama3-lora-train\")\n",
    "\n",
    "base_output_dir = os.path.join(STAGING_BUCKET, job_name)\n",
    "# Create a GCS folder to store the LORA adapter.\n",
    "lora_output_dir = os.path.join(base_output_dir, \"adapter\")\n",
    "# Create a GCS folder to store the merged model with the base model and the\n",
    "# finetuned LORA adapter.\n",
    "merged_model_output_dir = os.path.join(base_output_dir, \"merged-model\")\n",
    "\n",
    "# Add labels for the finetuning job.\n",
    "labels = {\n",
    "    \"mg-source\": \"notebook\",\n",
    "    \"mg-notebook-name\": \"model_garden_pytorch_llama3_finetuning.ipynb\".split(\".\")[0],\n",
    "}\n",
    "\n",
    "labels[\"mg-tune\"] = \"publishers-meta-models-llama3\"\n",
    "versioned_model_id = base_model_id.split(\"/\")[1].lower().replace(\".\", \"-\")\n",
    "labels[\"versioned-mg-tune\"] = f\"{labels['mg-tune']}-{versioned_model_id}\"\n",
    "\n",
    "eval_args = [\n",
    "    f\"--eval_dataset_path={eval_dataset_name}\",\n",
    "    f\"--eval_column={instruct_column_in_dataset}\",\n",
    "    f\"--eval_template={template}\",\n",
    "    f\"--eval_split={eval_split_name}\",\n",
    "    f\"--eval_steps={save_steps}\",\n",
    "    \"--eval_tasks=builtin_eval\",\n",
    "    \"--eval_metric_name=loss\",\n",
    "]\n",
    "\n",
    "train_job_args = [\n",
    "    \"--config_file=vertex_vision_model_garden_peft/deepspeed_zero2_4gpu.yaml\",\n",
    "    \"--task=instruct-lora\",\n",
    "    \"--completion_only=True\",\n",
    "    f\"--pretrained_model_id={base_model_id}\",\n",
    "    f\"--dataset_name={train_dataset_name}\",\n",
    "    f\"--train_split_name={train_split_name}\",\n",
    "    f\"--instruct_column_in_dataset={instruct_column_in_dataset}\",\n",
    "    f\"--output_dir={lora_output_dir}\",\n",
    "    f\"--merge_base_and_lora_output_dir={merged_model_output_dir}\",\n",
    "    f\"--per_device_train_batch_size={per_device_train_batch_size}\",\n",
    "    f\"--gradient_accumulation_steps={gradient_accumulation_steps}\",\n",
    "    f\"--lora_rank={lora_rank}\",\n",
    "    f\"--lora_alpha={lora_alpha}\",\n",
    "    f\"--lora_dropout={lora_dropout}\",\n",
    "    f\"--max_steps={max_steps}\",\n",
    "    f\"--max_seq_length={max_seq_length}\",\n",
    "    f\"--learning_rate={learning_rate}\",\n",
    "    f\"--lr_scheduler_type={lr_scheduler_type}\",\n",
    "    f\"--precision_mode={finetuning_precision_mode}\",\n",
    "    f\"--enable_gradient_checkpointing={enable_gradient_checkpointing}\",\n",
    "    f\"--num_epochs={num_epochs}\",\n",
    "    f\"--attn_implementation={attn_implementation}\",\n",
    "    f\"--optimizer={optimizer}\",\n",
    "    f\"--warmup_ratio={warmup_ratio}\",\n",
    "    f\"--report_to={report_to}\",\n",
    "    f\"--logging_output_dir={base_output_dir}\",\n",
    "    f\"--save_steps={save_steps}\",\n",
    "    f\"--logging_steps={logging_steps}\",\n",
    "    f\"--template={template}\",\n",
    "    f\"--huggingface_access_token={HF_TOKEN}\",\n",
    "] + eval_args\n",
    "\n",
    "# Create TensorBoard\n",
    "tensorboard = aiplatform.Tensorboard.create(job_name)\n",
    "exp = aiplatform.TensorboardExperiment.create(\n",
    "    tensorboard_experiment_id=job_name, tensorboard_name=tensorboard.name\n",
    ")\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "train_job.run(\n",
    "    args=train_job_args,\n",
    "    environment_variables={\"WANDB_DISABLED\": True},\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=tensorboard.resource_name,\n",
    "    base_output_dir=base_output_dir,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapter was saved in: \", lora_output_dir)\n",
    "print(\"Trained and merged models were saved in: \", merged_model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run TensorBoard\n",
    " This section shows how to launch TensorBoard in a [Cloud Shell](https://cloud.google.com/shell/docs).\n",
    " 1. Click the Cloud Shell icon(![terminal](https://github.com/google/material-design-icons/blob/master/png/action/terminal/materialicons/24dp/1x/baseline_terminal_black_24dp.png?raw=true)) on the top right to open the Cloud Shell.\n",
    " 2. Copy the `tensorboard` command shown below by running this cell.\n",
    " 3. Paste and run the command in the Cloud Shell to launch TensorBoard.\n",
    " 4. Once the command runs (You may have to click `Authorize` if prompted), click the link starting with `http://localhost`.\n",
    "\n",
    " Note: You may need to wait around 10 minutes after the job starts in order for the TensorBoard logs to be written to the GCS bucket.\n",
    "print(f\"Command to copy: tensorboard --logdir {base_output_dir}/logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this code to reload your model\n",
    "# merged_model_output_dir = \"<REPLACE_WITH_MODEL_ARTIFACTS_GCS_URI>\" # E.g. gs://<BUCKET_NAME>/temporal/<JOB_NAME>/merged-model\"\n",
    "# MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "qmHW6m8xG_4U",
    "outputId": "93b8fc54-7923-4f27-9a4c-1c20539943e5"
   },
   "outputs": [],
   "source": [
    "print(\"Deploying models in: \", merged_model_output_dir)\n",
    "\n",
    "# The pre-built serving docker image for vLLM.\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240721_0916_RC00\"\n",
    "\n",
    "accelerator_type = \"NVIDIA_L4\"  # @param [\"NVIDIA_L4\", \"NVIDIA_H100_80GB\"]\n",
    "machine_type = None\n",
    "\n",
    "# Find Vertex AI prediction supported accelerators and regions in [here](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute).\n",
    "if \"8b\" in MODEL_ID.lower():\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-12\"\n",
    "        accelerator_count = 1\n",
    "    elif accelerator_type == \"NVIDIA_H100_80GB\":\n",
    "        machine_type = \"a3-highgpu-2g\"\n",
    "        accelerator_count = 2\n",
    "else:\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-96\"\n",
    "        accelerator_count = 8\n",
    "    elif accelerator_type == \"NVIDIA_H100_80GB\":\n",
    "        machine_type = \"a3-highgpu-4g\"\n",
    "        accelerator_count = 4\n",
    "\n",
    "if machine_type is None:\n",
    "    raise ValueError(\n",
    "        f\"Recommended GPU setting not found for: {accelerator_type} and {MODEL_ID.lower()}.\"\n",
    "    )\n",
    "\n",
    "common_util.check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    is_for_training=False,\n",
    ")\n",
    "\n",
    "gpu_memory_utilization = 0.85\n",
    "max_model_len = 8192  # Maximum context length.\n",
    "\n",
    "# Ensure max_model_len does not exceed the limit\n",
    "if max_model_len > 8192:\n",
    "    raise ValueError(\"max_model_len cannot exceed 8192\")\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    publisher: str,\n",
    "    publisher_model_id: str,\n",
    "    service_account: str,\n",
    "    base_model_id: str = None,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    gpu_memory_utilization: float = 0.9,\n",
    "    max_model_len: int = 4096,\n",
    "    dtype: str = \"auto\",\n",
    "    enable_trust_remote_code: bool = False,\n",
    "    enforce_eager: bool = False,\n",
    "    enable_lora: bool = False,\n",
    "    enable_chunked_prefill: bool = False,\n",
    "    enable_prefix_cache: bool = False,\n",
    "    host_prefix_kv_cache_utilization_target: float = 0.0,\n",
    "    max_loras: int = 1,\n",
    "    max_cpu_loras: int = 8,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    "    max_num_seqs: int = 256,\n",
    "    model_type: str = None,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=f\"{model_name}-endpoint\",\n",
    "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "    )\n",
    "\n",
    "    if not base_model_id:\n",
    "        base_model_id = model_id\n",
    "\n",
    "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
    "    vllm_args = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.api_server\",\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=8080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        f\"--max-loras={max_loras}\",\n",
    "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
    "        f\"--max-num-seqs={max_num_seqs}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "\n",
    "    if enable_trust_remote_code:\n",
    "        vllm_args.append(\"--trust-remote-code\")\n",
    "\n",
    "    if enforce_eager:\n",
    "        vllm_args.append(\"--enforce-eager\")\n",
    "\n",
    "    if enable_lora:\n",
    "        vllm_args.append(\"--enable-lora\")\n",
    "\n",
    "    if enable_chunked_prefill:\n",
    "        vllm_args.append(\"--enable-chunked-prefill\")\n",
    "\n",
    "    if enable_prefix_cache:\n",
    "        vllm_args.append(\"--enable-prefix-caching\")\n",
    "\n",
    "    if 0 < host_prefix_kv_cache_utilization_target < 1:\n",
    "        vllm_args.append(\n",
    "            f\"--host-prefix-kv-cache-utilization-target={host_prefix_kv_cache_utilization_target}\"\n",
    "        )\n",
    "\n",
    "    if model_type:\n",
    "        vllm_args.append(f\"--model-type={model_type}\")\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": base_model_id,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[8080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "        model_garden_source_model_name=(\n",
    "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
    "    )\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        system_labels={\n",
    "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_finetuning.ipynb\",\n",
    "            # \"NOTEBOOK_ENVIRONMENT\": \"notebook\",\n",
    "            \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
    "        },\n",
    "    )\n",
    "    print(\"endpoint_name:\", endpoint.name)\n",
    "\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "models[\"vllm_gpu\"], endpoints[\"vllm_gpu\"] = deploy_model_vllm(\n",
    "    model_name=common_util.get_job_name_with_datetime(prefix=\"llama3-vllm-serve\"),\n",
    "    model_id=merged_model_output_dir,\n",
    "    publisher=\"meta\",\n",
    "    publisher_model_id=\"llama3\",\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    "    max_model_len=max_model_len,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Human: What is a car?\n",
    "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "```\n",
    "Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
    "\n",
    "- If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
    "-  Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = \"\"  \n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(aip_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UYUNn60G_4U",
    "outputId": "468a32ce-d3e4-49de-bf6a-f353c6d63a2c"
   },
   "outputs": [],
   "source": [
    "prompt = \"What is a car?\"  \n",
    "max_tokens = 200  \n",
    "temperature = 1.0  \n",
    "top_p = 1.0  \n",
    "top_k = 1  \n",
    "raw_response = False  \n",
    "\n",
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"raw_response\": raw_response,\n",
    "    },\n",
    "]\n",
    "response = endpoints[\"vllm_gpu\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "## Clean up resources\n",
    "### Delete the model and endpoint\n",
    "Delete the experiment models and endpoints to recycle the resources and avoid unnecessary continuous charges that may incur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "train_job.delete()\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "for endpoint in endpoints.values():\n",
    "    endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "for model in models.values():\n",
    "    model.delete()\n",
    "\n",
    "delete_bucket = False  \n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_llama3_finetuning.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
