{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Fraudfinder - ML Pipeline\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/fraudfinder/blob/main/vertex_ai/06_formalization.ipynb\">\n",
    "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Google Cloud Notebooks\">Open in Cloud Notebook\n",
    "    </a>\n",
    "  </td> \n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/fraudfinder/blob/main/vertex_ai/06_formalization.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/fraudfinder/blob/main/vertex_ai/06_formalization.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Fraudfinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the Fraudfinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebook shows how to use Feature Store, Pipelines and Model Monitoring for building an end-to-end demo using both components defined in `google_cloud_pipeline_components` and custom components. \n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "- [BigQuery](https://cloud.google.com/bigquery/)\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "* Create a Vetex AI Pipeline to orchestrate and automate the ML workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    with open(\"../config_path.json\", \"r\") as f:\n",
    "        config_path = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"config_path.json not found. Please make sure the file exists.\")\n",
    "    ID = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_from_bucket, VertexConfig\n",
    "\n",
    "\n",
    "config = read_from_bucket(config_path[\"bucket\"], config_path[\"conf_uri\"])\n",
    "config = VertexConfig(**config)\n",
    "\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$config.PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf",
    "tags": []
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries\n",
    "Next you will import the libraries needed for this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that currently this notebook uses KFP SDK v1, whereas the environment includes KFP v2. As an interim solution, we will downlevel KFP and the Google Cloud Pipeline Components in order to use the v1 code here as-is. See the [KFP migration guide](https://www.kubeflow.org/docs/components/pipelines/v2/migration/) for more details of moving from v1 to v2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pRUOFELefqf1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# Vertex Pipelines\n",
    "import kfp\n",
    "from kfp import dsl, compiler\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google_cloud_pipeline_components.v1 import dataset, custom_job, endpoint\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components._placeholders import PERSISTENT_RESOURCE_ID_PLACEHOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "print(\"kfp version:\", kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Components variables\n",
    "COMPONENTS_DIR = os.path.join(os.curdir, \"pipelines\", \"components\")\n",
    "INGEST_FEATURE_STORE = f\"{COMPONENTS_DIR}/ingest_feature_store_{config.ID}.yaml\"\n",
    "TRAIN_MODEL = f\"{COMPONENTS_DIR}/train_model_{config.ID}.yaml\"\n",
    "EVALUATE = f\"{COMPONENTS_DIR}/evaluate_{config.ID}.yaml\"\n",
    "\n",
    "# Pipeline variables\n",
    "PIPELINE_DIR = os.path.join(os.curdir, \"pipelines\")\n",
    "PIPELINE_PACKAGE_PATH = f\"{PIPELINE_DIR}/pipeline_{config.ID}.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Vertex AI SDK\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vertex AI SDK\n",
    "vertex_ai.init(\n",
    "    project=config.PROJECT_ID,\n",
    "    location=config.REGION,\n",
    "    staging_bucket=config.BUCKET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Uniform bucket-level access for gs://fraud-finder-lab-fraudfinder...\n"
     ]
    }
   ],
   "source": [
    "!gsutil ubla set on gs://{vertex_config.BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create directories \n",
    "Create a directory for you pipeline and pipeline components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p -m 777 $PIPELINE_DIR $COMPONENTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a end-to-end Pipeline and execute it on Vertex AI Pipelines.\n",
    "\n",
    "We will build a pipeline that you will execute using [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction). Vertex AI Pipelines helps you to automate, monitor, and govern your ML systems by orchestrating your ML workflow in a serverless manner, and storing your workflow's artifacts using Vertex ML Metadata. Authoring ML Pipelines that run on Vertex AI pipelines can be done in two different ways:\n",
    "\n",
    "* [Tensorflow Extended](https://www.tensorflow.org/tfx/guide)\n",
    "* [Kubeflow Pipelines SDK](https://kubeflow-pipelines.readthedocs.io/en/1.8.13/)\n",
    "\n",
    "Based on your preference you can choose between the two options. This notebook will only focus on Kubeflow Pipelines.\n",
    "\n",
    "If you don't have familiarity in authoring pipelines in Vertex AI Pipelines, we suggest the following resources:\n",
    "* [Introduction to Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)\n",
    "* [Build a Pipeline in Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Components for your pipeline\n",
    "\n",
    "We will use a mix of prebuilt (Google Cloud Pipeline Components) and custom components in this notebook. The difference is:\n",
    "\n",
    "* Prebuilt components are official [Google Cloud Pipeline Components](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction)(GCPC). The GCPC Library provides a set of prebuilt components that are production quality, consistent, performant, and easy to use in Vertex AI Pipelines.\n",
    "* As you will build in the cell below, a data scientist or ML engineer typically authored the custom component. This means you have more control over the component (container) code. In this case, it's a Python-function-based component. You also have the option to build a component yourself by packaging code into a container.\n",
    "\n",
    "In the following two cells, you will build two custom components:\n",
    "\n",
    "    *Feature Store component.\n",
    "\n",
    "    *Evaluation component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define feature store component\n",
    "\n",
    "Notice that the component assumes that contains the entities-timestamps \"query\" is already created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Store\n",
    "Next you will build a custom component using the [KFP SDK](https://kubeflow-pipelines.readthedocs.io/en/1.8.13/). Here you will take a Python function and create a component out of it. This component will take features from the Vertex AI Feature Store and output them on Google Cloud Storage (GCS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=config.BASE_IMAGE,\n",
    "    packages_to_install=[\n",
    "        \"gcsfs==2025.3.2\",\n",
    "        \"google-cloud-aiplatform==1.88.0\",\n",
    "        \"google-cloud-bigquery==3.26.0\",\n",
    "        \"bigframes==1.42.0\",\n",
    "        \"pandas==2.2.3\",\n",
    "        \"Jinja2==3.1.6\",\n",
    "    ],\n",
    ")\n",
    "def ingest_features_gcs(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    bucket_name: str,\n",
    "    read_instances_table: str,\n",
    ") -> str:\n",
    "    # Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "    from datetime import datetime\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    import bigframes\n",
    "    import bigframes.pandas\n",
    "    from google.cloud import bigquery\n",
    "    from vertexai.resources.preview.feature_store import FeatureGroup, offline_store\n",
    "\n",
    "\n",
    "    # Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    export_data_dir = f\"/gcs/{bucket_name}/data/snapshots/{timestamp}\"\n",
    "    export_data_path = f\"{export_data_dir}/000000.csv\"\n",
    "    customer_entity = \"customer\"\n",
    "    terminal_entity = \"terminal\"\n",
    "    customer_features_str = [\n",
    "        \"customer_id_nb_tx_1day_window\",\n",
    "        \"customer_id_nb_tx_7day_window\",\n",
    "        \"customer_id_nb_tx_14day_window\",\n",
    "        \"customer_id_avg_amount_1day_window\",\n",
    "        \"customer_id_avg_amount_7day_window\",\n",
    "        \"customer_id_avg_amount_14day_window\",\n",
    "        \"customer_id_nb_tx_15min_window\",\n",
    "        \"customer_id_nb_tx_30min_window\",\n",
    "        \"customer_id_nb_tx_60min_window\",\n",
    "        \"customer_id_avg_amount_15min_window\",\n",
    "        \"customer_id_avg_amount_30min_window\",\n",
    "        \"customer_id_avg_amount_60min_window\",\n",
    "    ]\n",
    "    terminal_features_str = [\n",
    "        \"terminal_id_nb_tx_1day_window\",\n",
    "        \"terminal_id_nb_tx_7day_window\",\n",
    "        \"terminal_id_nb_tx_14day_window\",\n",
    "        \"terminal_id_risk_1day_window\",\n",
    "        \"terminal_id_risk_7day_window\",\n",
    "        \"terminal_id_risk_14day_window\",\n",
    "        \"terminal_id_nb_tx_15min_window\",\n",
    "        \"terminal_id_nb_tx_30min_window\",\n",
    "        \"terminal_id_nb_tx_60min_window\",\n",
    "        \"terminal_id_avg_amount_15min_window\",\n",
    "        \"terminal_id_avg_amount_30min_window\",\n",
    "        \"terminal_id_avg_amount_60min_window\",\n",
    "    ]\n",
    "    read_instances_query = f\"\"\"\n",
    "        SELECT\n",
    "            gt.timestamp,\n",
    "            gt.customer_id,\n",
    "            gt.terminal_id,\n",
    "            gt.tx_amount,\n",
    "            gt.tx_fraud,\n",
    "        FROM \n",
    "            `{project_id}.tx.{read_instances_table}` as gt;\n",
    "    \"\"\"\n",
    "\n",
    "    # Main -------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    ## Run batch job request\n",
    "    # get instances to fetch features\n",
    "    print(\"ingest_features_gcs: query\", read_instances_query)\n",
    "    bq_client = bigquery.Client(project=project_id, location=location)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(read_instances_query, job_config=job_config)\n",
    "    # Wait for query/job to finish running. then get & return data frame\n",
    "    instances_df = client_result.result().to_arrow().to_pandas()\n",
    "    print(\"ingest_features_gcs - instances_df\", instances_df.shape)\n",
    "\n",
    "    # read from feature store\n",
    "    customer_fg = FeatureGroup(name=customer_entity)\n",
    "    customer_features = [customer_fg.get_feature(c_feat) for c_feat in customer_features_str]\n",
    "    terminal_fg = FeatureGroup(name=terminal_entity)\n",
    "    terminal_features = [terminal_fg.get_feature(t_feat) for t_feat in terminal_features_str]\n",
    "    sample_df = offline_store.fetch_historical_feature_values(\n",
    "        entity_df=instances_df,\n",
    "        features=customer_features + terminal_features,\n",
    "    )\n",
    "    sample_df = sample_df.to_pandas()\n",
    "    print(\"ingest_features_gcs - sample_df\", sample_df.shape)\n",
    "    # vertex AI pipeline support cloud storage FUSE\n",
    "    if not Path(export_data_dir).exists():\n",
    "        Path(export_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    sample_df.to_csv(export_data_path, index=False)\n",
    "\n",
    "    # Store metadata\n",
    "    snapshot_files_fmt = [export_data_path.replace(\"/gcs/\", \"gs://\")]\n",
    "    snapshot_files_string = json.dumps(snapshot_files_fmt)\n",
    "    print(\"ingest_features_gcs - snapshot_files_string\", snapshot_files_string)\n",
    "\n",
    "    return snapshot_files_string\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(ingest_features_gcs, INGEST_FEATURE_STORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom training\n",
    "You will package the training job like in previous module 05 as a custom training job component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=config.BASE_IMAGE,\n",
    "    packages_to_install=[\n",
    "        \"gcsfs==2025.3.2\",\n",
    "        \"tqdm==4.67.1\",\n",
    "        \"numpy==2.2.4\",\n",
    "        \"pandas==2.2.3\",\n",
    "        \"torch==2.6.0\",\n",
    "        \"dask==2025.3.0\",\n",
    "        \"dask-ml==2025.1.0\",\n",
    "        \"distributed==2025.3.0\",\n",
    "        \"google-cloud-pipeline-components==2.17.0\",\n",
    "        \"google-cloud-aiplatform==1.88.0\",\n",
    "    ]\n",
    ")\n",
    "def train_model(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    bucket: str,\n",
    "    dataset: dsl.Input[artifact_types.VertexDataset],\n",
    "    dtype: dict,\n",
    "    drop_cols: List[str],\n",
    "    target_col: str,\n",
    "    feat_cols: List[str],\n",
    "    model_reg: str,\n",
    "    model_serving_image_uri: str,\n",
    "    trained_model: dsl.Output[artifact_types.VertexModel],\n",
    "    test_ds: dsl.Output[dsl.Dataset],\n",
    "):\n",
    "    import os\n",
    "    from typing import List\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime, timezone\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tqdm\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.autograd import Variable\n",
    "    import dask.dataframe as dask_df\n",
    "    from dask_ml.model_selection import train_test_split\n",
    "\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud.aiplatform import explain\n",
    "\n",
    "\n",
    "\n",
    "    ## Read environmental variables\n",
    "    def gcs_path_to_local_path(old_path: str) -> str:\n",
    "        new_path = old_path.replace(\"gs://\", \"/gcs/\")\n",
    "        return new_path\n",
    "\n",
    "    def resample(df: pd.DataFrame, replace: bool, frac: float = 1, random_state: int = 8) -> pd.DataFrame:\n",
    "        shuffled_df = df.sample(frac=frac, replace=replace, random_state=random_state)\n",
    "        return shuffled_df\n",
    "\n",
    "    def preprocess(df: pd.DataFrame, drop_cols: List[str] = None) -> pd.DataFrame:\n",
    "        if drop_cols:\n",
    "            df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "        # Drop rows with NaN\"s\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Convert integer valued (numeric) columns to floating point\n",
    "        numeric_columns = df.select_dtypes([\"float32\", \"float64\"]).columns\n",
    "        numeric_format = {col:\"float32\" for col in numeric_columns}\n",
    "        df = df.astype(numeric_format)\n",
    "\n",
    "        return df\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super().__init__()\n",
    "            self.l1 = nn.Linear(input_dim, 100)\n",
    "            self.l2 = nn.Linear(100, 30)\n",
    "            self.l3 = nn.Linear(30, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.l1(x))\n",
    "            x = F.relu(self.l2(x))\n",
    "            x = F.relu(self.l3(x))\n",
    "            x = F.softmax(x, dim=1)\n",
    "            return x\n",
    "\n",
    "    ## Training variables\n",
    "    N_PARTITIONS = 4\n",
    "    vertex_ai.init(project=project, location=location, staging_bucket=f\"gs://{bucket}\")\n",
    "\n",
    "    # manually extract and split \n",
    "    dataset_id = dataset.metadata['resourceName'].split(\"/\")[-1]\n",
    "    dataset = vertex_ai.TabularDataset(dataset.metadata['resourceName'])\n",
    "    dataset_uris = dataset.gca_resource.metadata['inputConfig']['gcsSource']['uri']\n",
    "    dataset_uris = [gcs_path_to_local_path(dataset_uri) for dataset_uri in dataset_uris]\n",
    "    print(\"train_model - dataset_uris\", dataset_uris)\n",
    "    ds_df = dask_df.read_csv(dataset_uris, dtype=dtype)\n",
    "    train_df, test_df = train_test_split(ds_df, test_size=0.2, shuffle=True)\n",
    "    eval_df, test_df = train_test_split(test_df, test_size=0.5)\n",
    "    TRAINING_DIR = (\n",
    "        f\"/gcs/{bucket}/aiplatform-custom-training-\"\n",
    "        f\"{datetime.now(timezone.utc).strftime('%Y-%m-%d-%H:%M:%S.%f')}\"\n",
    "    )\n",
    "    TRAINING_DATA_DIR = (\n",
    "        f\"{TRAINING_DIR}/dataset-{dataset_id}-tables-\"\n",
    "        f\"{datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%S.%fZ')}\"\n",
    "    )\n",
    "    TRAINING_DATA_PATH = f\"{TRAINING_DATA_DIR}/training-0000*-of-0000{N_PARTITIONS}.csv\"\n",
    "    EVAL_DATA_PATH = f\"{TRAINING_DATA_DIR}/validation-0000*-of-0000{N_PARTITIONS}.csv\"\n",
    "    TEST_DATA_PATH = f\"{TRAINING_DATA_DIR}/test-0000*-of-0000{N_PARTITIONS}.csv\"\n",
    "    train_df.repartition(npartitions=N_PARTITIONS).to_csv(TRAINING_DATA_PATH)\n",
    "    eval_df.repartition(npartitions=N_PARTITIONS).to_csv(EVAL_DATA_PATH)\n",
    "    test_df.repartition(npartitions=N_PARTITIONS).to_csv(TEST_DATA_PATH)\n",
    "    print(\"train_model - dataset prepared\")\n",
    "    MODEL_DIR = f\"{TRAINING_DIR}/model\"\n",
    "    MODEL_PATH = f\"{MODEL_DIR}/model.pt\"\n",
    "\n",
    "    # preprocessing\n",
    "    train_df = train_df.compute()\n",
    "    test_df = test_df.compute()\n",
    "    preprocessed_train_df = preprocess(train_df, drop_cols)\n",
    "    preprocessed_test_df = preprocess(test_df, drop_cols)\n",
    "    \n",
    "    # downsampling\n",
    "    train_nfraud_df = preprocessed_train_df[preprocessed_train_df[target_col]==0]\n",
    "    train_fraud_df = preprocessed_train_df[preprocessed_train_df[target_col]==1]\n",
    "    train_nfraud_downsample = resample(\n",
    "        train_nfraud_df,\n",
    "        replace=False, \n",
    "        frac=len(train_fraud_df)/len(train_df)\n",
    "    )\n",
    "    ds_preprocessed_train_df = pd.concat([train_nfraud_downsample, train_fraud_df])\n",
    "    print(\"train_model - ds_preprocessed_train_df\", ds_preprocessed_train_df.shape)\n",
    "    # Train torch model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # target, features split\n",
    "    x_train = ds_preprocessed_train_df[feat_cols].astype(np.float32).values\n",
    "    x_train = Variable(torch.from_numpy(x_train)).float().to(device)\n",
    "    y_train = ds_preprocessed_train_df.loc[:, target_col].astype(int).values\n",
    "    y_train = Variable(torch.from_numpy(y_train)).long().to(device)\n",
    "    x_true = preprocessed_test_df[feat_cols].astype(np.float32).values\n",
    "    x_true = Variable(torch.from_numpy(x_true)).float().to(device)\n",
    "    y_true = preprocessed_test_df.loc[:, target_col].astype(int).values\n",
    "    y_true = Variable(torch.from_numpy(y_true)).long().to(device)\n",
    "    preprocessed_test_dask_df = dask_df.from_pandas(preprocessed_test_df, npartitions=4)\n",
    "    preprocessed_test_dask_df.to_csv(os.path.join(test_ds.path, \"test-0000*-of-0000{N_PARTITIONS}.csv\"))\n",
    "    print(\"train_model - preprocessed_test_df\", preprocessed_test_df.shape)\n",
    "    # start training; for demo purpose, no validation/early stopping is implemented\n",
    "    EPOCHS = 5\n",
    "    model = Model(x_train.shape[1]).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for _ in tqdm.trange(EPOCHS):\n",
    "        y_pred = model(x_train)\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if not Path(MODEL_DIR).exists():\n",
    "        Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    model_jit = torch.jit.trace(model, example_kwarg_inputs={\"x\": torch.from_numpy(np.random.random((10, x_train.shape[1]))).float()})\n",
    "    torch.jit.save(model_jit, MODEL_PATH)\n",
    "    print(\"train_model - jit model saved\")\n",
    "    # upload model\n",
    "    explanation_params = explain.ExplanationParameters(\n",
    "        sampled_shapley_attribution=explain.SampledShapleyAttribution(\n",
    "            path_count=10,\n",
    "        ),\n",
    "    )\n",
    "    explanation_metadata = explain.ExplanationMetadata(\n",
    "        inputs={\n",
    "            feat: {} for feat in feat_cols\n",
    "        },\n",
    "        outputs={\n",
    "            \"probability\": {}\n",
    "        }\n",
    "    )\n",
    "    vertex_ai_model = vertex_ai.Model.upload(\n",
    "        serving_container_image_uri=model_serving_image_uri,\n",
    "        artifact_uri=MODEL_DIR.replace(\"/gcs/\", \"gs://\"),\n",
    "        display_name=model_reg,\n",
    "        description=\"Vertex AI Pipeline Custom Model\",\n",
    "        explanation_metadata=explanation_metadata,\n",
    "        explanation_parameters=explanation_params,\n",
    "    )\n",
    "    trained_model.uri = vertex_ai_model.uri\n",
    "    trained_model.metadata[\"resourceName\"] = vertex_ai_model.resource_name\n",
    "    trained_model.metadata[\"path\"] = MODEL_PATH\n",
    "    print(trained_model.metadata)\n",
    "    print(trained_model.path, trained_model.uri)\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(train_model, TRAIN_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define an evaluate custom component\n",
    "Next you will build a custom component that will evaluate our pytorch model. This component will output `avg_precision_score` so that it can be used downstream for validating the model before deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=config.BASE_IMAGE,\n",
    "    packages_to_install=[\n",
    "        \"gcsfs==2025.3.2\",\n",
    "        \"numpy==2.2.4\",\n",
    "        \"pandas==2.2.3\",\n",
    "        \"torch==2.6.0\",\n",
    "        \"dask[dataframe]==2025.3.0\",\n",
    "        \"pyarrow==19.0.1\",\n",
    "        \"distributed==2025.3.0\",\n",
    "        \"google-cloud-pipeline-components==2.17.0\",\n",
    "        \"google-cloud-aiplatform==1.88.0\",\n",
    "        \"scikit-learn==1.6.1\",\n",
    "    ],\n",
    ")\n",
    "def evaluate_model(\n",
    "    threshold: float,\n",
    "    model_in: dsl.Input[artifact_types.VertexModel],\n",
    "    test_ds: dsl.Input[dsl.Dataset],\n",
    "    dtype: dict,\n",
    "    target_col: str,\n",
    "    feat_cols: List[str],\n",
    "    metrics_uri: str,\n",
    ") -> NamedTuple(\n",
    "    \"outputs\",\n",
    "    meta_metrics=dsl.Metrics,\n",
    "    graph_metrics=dsl.ClassificationMetrics,\n",
    "    avg_prec=float,\n",
    "):\n",
    "    # Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    import dask.dataframe as dask_df\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from sklearn.metrics import (confusion_matrix, average_precision_score, f1_score, \n",
    "                                log_loss, precision_score, recall_score)\n",
    "\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "\n",
    "    def evaluate_model(model: nn.Module, x_true: np.ndarray, y_true: np.ndarray | pd.Series, thresh: float) -> dict:\n",
    "        #calculate metrics\n",
    "        metrics={}\n",
    "        \n",
    "        x_true = torch.from_numpy(x_true.astype(np.float32)).to(device)\n",
    "        y_true = y_true.astype(int)\n",
    "        y_score = model(x_true)[:, 1].detach().numpy()\n",
    "        y_pred = np.where(y_score >= thresh, 1, 0)\n",
    "        c_matrix = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        avg_precision_score = round(average_precision_score(y_true, y_score), 3)\n",
    "        f1 = round(f1_score(y_true, y_pred), 3)\n",
    "        lg_loss = round(log_loss(y_true, y_pred), 3)\n",
    "        prec_score = round(precision_score(y_true, y_pred), 3)\n",
    "        rec_score = round(recall_score(y_true, y_pred), 3)\n",
    "        \n",
    "        metrics[\"confusion_matrix\"] = c_matrix.tolist()\n",
    "        metrics[\"avg_precision_score\"] = avg_precision_score\n",
    "        metrics[\"f1_score\"] = f1\n",
    "        metrics[\"log_loss\"] = lg_loss\n",
    "        metrics[\"precision_score\"] = prec_score\n",
    "        metrics[\"recall_score\"] = rec_score\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "    # load the dataframe, dask save to path as folder, need to put wildcard\n",
    "    print(\"eval\", test_ds.path)\n",
    "    print(\"eval\", model_in.path)\n",
    "    test_df = dask_df.read_csv(f\"{test_ds.path}/*\", dtype=dtype)\n",
    "    test_df = test_df.compute()\n",
    "    model = torch.jit.load(model_in.metadata[\"path\"])\n",
    "    eval_metrics = evaluate_model(model, test_df[feat_cols].values, test_df[target_col].values, thresh=threshold)\n",
    "\n",
    "    # Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "    metrics_path = metrics_uri.replace(\"gs://\", \"/gcs/\")\n",
    "    labels = [\"not fraud\", \"fraud\"]\n",
    "\n",
    "    # Main -------------------------------------------------------------------------------------------------------------------------------\n",
    "    metrics_path_dir = Path(metrics_path).parent\n",
    "    if not metrics_path_dir.exists():\n",
    "        metrics_path_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(metrics_path, mode=\"w\") as metrics_file:\n",
    "        json.dump(eval_metrics, metrics_file, indent=2)\n",
    "\n",
    "    ## metrics\n",
    "    c_matrix = eval_metrics[\"confusion_matrix\"]\n",
    "    avg_precision_score = eval_metrics[\"avg_precision_score\"]\n",
    "    f1 = eval_metrics[\"f1_score\"]\n",
    "    lg_loss = eval_metrics[\"log_loss\"]\n",
    "    prec_score = eval_metrics[\"precision_score\"]\n",
    "    rec_score = eval_metrics[\"recall_score\"]\n",
    "\n",
    "    meta_metrics = dsl.Metrics()\n",
    "    meta_metrics.log_metric(\"avg_precision_score\", avg_precision_score)\n",
    "    meta_metrics.log_metric(\"f1_score\", f1)\n",
    "    meta_metrics.log_metric(\"log_loss\", lg_loss)\n",
    "    meta_metrics.log_metric(\"precision_score\", prec_score)\n",
    "    meta_metrics.log_metric(\"recall_score\", rec_score)\n",
    "    graph_metrics = dsl.ClassificationMetrics()\n",
    "    graph_metrics.log_confusion_matrix(labels, c_matrix)\n",
    "\n",
    "\n",
    "    ## model metadata\n",
    "    # model_out.metadata[\"framework\"] = \"torch\"\n",
    "    # model_out.metadata[\"algorithm\"] = \"FNN\"\n",
    "    # model_out.metadata[\"type\"] = \"classification\"\n",
    "    print(\"metadata metrics\", meta_metrics.metadata)\n",
    "    print(\"graph metrics\", graph_metrics.metadata)\n",
    "\n",
    "    eval_output = NamedTuple(\n",
    "        \"outputs\",\n",
    "        meta_metrics=dsl.Metrics,\n",
    "        graph_metrics=dsl.ClassificationMetrics,\n",
    "        avg_prec=float, \n",
    "    )\n",
    "    return eval_output(\n",
    "        meta_metrics=meta_metrics,\n",
    "        graph_metrics=graph_metrics,\n",
    "        avg_prec=avg_precision_score,\n",
    "    )\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(evaluate_model, EVALUATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author your pipeline\n",
    "Next you will author the pipeline using the KFP SDK. This pipeline consists of the following steps:\n",
    "\n",
    "* Ingest features\n",
    "* Create Vertex AI Dataset\n",
    "* Train Pytorch model\n",
    "* Evaluate model\n",
    "* Condition\n",
    "* Create endpoint\n",
    "* Deploy model into endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=config.PIPELINE_ROOT,\n",
    "    name=config.PIPELINE_NAME,\n",
    ")\n",
    "def pipeline(\n",
    "    project_id: str = config.PROJECT_ID,\n",
    "    location: str = config.REGION,\n",
    "    bucket_name: str = config.BUCKET_NAME,\n",
    "    deploy_machine_type: str = config.DEPLOY_COMPUTE,\n",
    "    metrics_uri: str = config.METRICS_URI,\n",
    "    model_threshold: float = config.MODEL_THRESHOLD,\n",
    "    thold: float = config.AVG_PR_THRESHOLD,\n",
    "):\n",
    "    # Ingest data from featurestore\n",
    "    ingest_features_op = ingest_features_gcs(\n",
    "        project_id=project_id,\n",
    "        location=location,\n",
    "        bucket_name=bucket_name,\n",
    "        read_instances_table=config.READ_INSTANCES_TABLE,\n",
    "    )\n",
    "\n",
    "    # create dataset\n",
    "    dataset_create_op = dataset.TabularDatasetCreateOp(\n",
    "        display_name=config.DATASET_NAME,\n",
    "        project=project_id,\n",
    "        gcs_source=ingest_features_op.output,\n",
    "    ).after(ingest_features_op)\n",
    "\n",
    "    # custom training job component - script\n",
    "    persistence_resource_id = (\n",
    "        config.PERSISTENT_RESOURCE_ID if config.PERSISTENT_RESOURCE_ID \n",
    "        else PERSISTENT_RESOURCE_ID_PLACEHOLDER\n",
    "    )\n",
    "    service_account = config.SERVICE_ACCOUNT if config.SERVICE_ACCOUNT else \"\"\n",
    "    train_model_component = custom_job.create_custom_training_job_from_component(\n",
    "        train_model,\n",
    "        display_name=config.JOB_NAME,\n",
    "        replica_count=config.REPLICA_COUNT,\n",
    "        machine_type=config.TRAIN_COMPUTE,\n",
    "        base_output_directory=f\"gs://{config.BUCKET_NAME}\",\n",
    "        service_account=service_account,\n",
    "        persistent_resource_id=persistence_resource_id,\n",
    "    )\n",
    "    train_model_op = train_model_component(\n",
    "        project=project_id,\n",
    "        location=config.REGION,\n",
    "        bucket=config.BUCKET_NAME,\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        dtype=config.DATA_SCHEMA,\n",
    "        drop_cols=config.DROP_COLUMNS,\n",
    "        target_col=config.TARGET_COLUMN,\n",
    "        feat_cols=config.FEAT_COLUMNS,\n",
    "        model_reg=config.MODEL_REGISTRY,\n",
    "        model_serving_image_uri=config.MODEL_SERVING_IMAGE_URI,\n",
    "    ).after(dataset_create_op)\n",
    "\n",
    "    # evaluate component\n",
    "    evaluate_model_op = evaluate_model(\n",
    "        threshold=model_threshold,\n",
    "        model_in=train_model_op.outputs[\"trained_model\"], \n",
    "        test_ds=train_model_op.outputs[\"test_ds\"],\n",
    "        dtype=config.DATA_SCHEMA,\n",
    "        target_col=config.TARGET_COLUMN,\n",
    "        feat_cols=config.FEAT_COLUMNS,\n",
    "        metrics_uri=metrics_uri,\n",
    "    ).after(train_model_op)\n",
    "\n",
    "    # if threshold on avg_precision_score\n",
    "    with dsl.If(\n",
    "        evaluate_model_op.outputs[\"avg_prec\"] > thold, name=config.AVG_PR_CONDITION\n",
    "    ):\n",
    "        # create endpoint\n",
    "        create_endpoint_op = endpoint.EndpointCreateOp(\n",
    "            display_name=f\"{config.ENDPOINT_NAME}_torch_pipeline_{config.ID}\",\n",
    "            project=project_id,\n",
    "            location=config.REGION,\n",
    "        ).after(evaluate_model_op)\n",
    "\n",
    "        # deploy the model\n",
    "        custom_model_deploy_op = endpoint.ModelDeployOp(\n",
    "            model=train_model_op.outputs[\"trained_model\"],\n",
    "            endpoint=create_endpoint_op.outputs[\"endpoint\"],\n",
    "            deployed_model_display_name=f\"{config.MODEL_NAME}_torch_pipeline_{config.ID}\",\n",
    "            dedicated_resources_machine_type=deploy_machine_type,\n",
    "            dedicated_resources_min_replica_count=config.REPLICA_COUNT,\n",
    "        ).after(create_endpoint_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and run the pipeline\n",
    "After authoring the pipeline you can use the compiler to compile the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile the pipeline\n",
    "pipeline_compiler = compiler.Compiler()\n",
    "pipeline_compiler.compile(pipeline_func=pipeline, package_path=PIPELINE_PACKAGE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you can use the Vertex AI SDK to create a job on Vertex AI Pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate pipeline representation\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    display_name=config.PIPELINE_NAME,\n",
    "    template_path=PIPELINE_PACKAGE_PATH,\n",
    "    pipeline_root=config.PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    "    project=config.PROJECT_ID,\n",
    "    location=config.REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job.run(\n",
    "    service_account=config.SERVICE_ACCOUNT,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test the endpoint when the model has completed deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "endpoint = vertex_ai.Endpoint(\"projects/<project-num>/locations/us-central1/endpoints/<endpoint-id>\")\n",
    "\n",
    "payload = {\n",
    "    \"instances\": [{feat: val for feat, val in zip(config.FEAT_COLUMNS, vals)} for vals in np.random.random((2, len(config.FEAT_COLUMNS))).tolist()]\n",
    "}\n",
    "resp = endpoint.explain(payload[\"instances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(predictions=[{'class': 'is_fraud', 'probability': 0.5399022102355957}, {'class': 'is_fraud', 'probability': 0.5536338686943054}], deployed_model_id='774763171930963968', metadata=None, model_version_id=None, model_resource_name=None, explanations=[attributions {\n",
      "  baseline_output_value: 0.54401206970214844\n",
      "  instance_output_value: 0.5399022102355957\n",
      "  feature_attributions {\n",
      "    struct_value {\n",
      "      fields {\n",
      "        key: \"tx_amount\"\n",
      "        value {\n",
      "          number_value: 0.00057162642478942867\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_risk_7day_window\"\n",
      "        value {\n",
      "          number_value: 0.003065508604049682\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_risk_1day_window\"\n",
      "        value {\n",
      "          number_value: -0.0004806816577911377\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_risk_14day_window\"\n",
      "        value {\n",
      "          number_value: -0.00096499323844909666\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_nb_tx_7day_window\"\n",
      "        value {\n",
      "          number_value: 0.0021901965141296392\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_nb_tx_60min_window\"\n",
      "        value {\n",
      "          number_value: 0.0027027547359466551\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_nb_tx_30min_window\"\n",
      "        value {\n",
      "          number_value: 9.3376636505126948e-05\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_nb_tx_1day_window\"\n",
      "        value {\n",
      "          number_value: 0.000349116325378418\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_nb_tx_15min_window\"\n",
      "        value {\n",
      "          number_value: 0.00038573145866394038\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_nb_tx_14day_window\"\n",
      "        value {\n",
      "          number_value: -0.0053600549697875978\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_avg_amount_60min_window\"\n",
      "        value {\n",
      "          number_value: -2.158880233764648e-05\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_avg_amount_30min_window\"\n",
      "        value {\n",
      "          number_value: -0.0032458901405334468\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_avg_amount_15min_window\"\n",
      "        value {\n",
      "          number_value: -0.00030075907707214348\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_nb_tx_7day_window\"\n",
      "        value {\n",
      "          number_value: -0.0016337513923645019\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_nb_tx_60min_window\"\n",
      "        value {\n",
      "          number_value: 0.00046591758728027339\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_nb_tx_30min_window\"\n",
      "        value {\n",
      "          number_value: 0.00116727352142334\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_nb_tx_1day_window\"\n",
      "        value {\n",
      "          number_value: -0.00063142776489257815\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_nb_tx_15min_window\"\n",
      "        value {\n",
      "          number_value: -0.00082026720046997066\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_nb_tx_14day_window\"\n",
      "        value {\n",
      "          number_value: 0.001669728755950928\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_avg_amount_7day_window\"\n",
      "        value {\n",
      "          number_value: -0.0014142692089080811\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_avg_amount_60min_window\"\n",
      "        value {\n",
      "          number_value: -0.00084720849990844722\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_avg_amount_30min_window\"\n",
      "        value {\n",
      "          number_value: -0.00044720172882080079\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_avg_amount_1day_window\"\n",
      "        value {\n",
      "          number_value: 0.00092567205429077153\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_avg_amount_15min_window\"\n",
      "        value {\n",
      "          number_value: -0.0016754686832427979\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_avg_amount_14day_window\"\n",
      "        value {\n",
      "          number_value: 0.0001468002796173096\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  output_index: -1\n",
      "  approximation_error: 0.0086567643944663823\n",
      "  output_name: \"probability\"\n",
      "}\n",
      ", attributions {\n",
      "  baseline_output_value: 0.54401206970214844\n",
      "  instance_output_value: 0.55363386869430542\n",
      "  feature_attributions {\n",
      "    struct_value {\n",
      "      fields {\n",
      "        key: \"tx_amount\"\n",
      "        value {\n",
      "          number_value: 0.0008452177047729492\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_risk_7day_window\"\n",
      "        value {\n",
      "          number_value: 0.002238988876342773\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_risk_1day_window\"\n",
      "        value {\n",
      "          number_value: 0.000688183307647705\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_risk_14day_window\"\n",
      "        value {\n",
      "          number_value: -0.00022177696228027339\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_nb_tx_7day_window\"\n",
      "        value {\n",
      "          number_value: -0.00093467235565185551\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_nb_tx_60min_window\"\n",
      "        value {\n",
      "          number_value: 0.012053155899047849\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_nb_tx_30min_window\"\n",
      "        value {\n",
      "          number_value: -8.9287757873535163e-06\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_nb_tx_1day_window\"\n",
      "        value {\n",
      "          number_value: 0.0030218005180358892\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_nb_tx_15min_window\"\n",
      "        value {\n",
      "          number_value: 0.0018654406070709229\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_nb_tx_14day_window\"\n",
      "        value {\n",
      "          number_value: -0.0086687028408050544\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_avg_amount_60min_window\"\n",
      "        value {\n",
      "          number_value: -1.6844272613525389e-05\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_avg_amount_30min_window\"\n",
      "        value {\n",
      "          number_value: -0.002037334442138672\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"terminal_id_avg_amount_15min_window\"\n",
      "        value {\n",
      "          number_value: -0.0036512792110443121\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_nb_tx_7day_window\"\n",
      "        value {\n",
      "          number_value: -0.00061879158020019533\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_nb_tx_60min_window\"\n",
      "        value {\n",
      "          number_value: -3.6090612411499017e-05\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_nb_tx_30min_window\"\n",
      "        value {\n",
      "          number_value: 0.0027780950069427488\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_nb_tx_1day_window\"\n",
      "        value {\n",
      "          number_value: 0.0002536773681640625\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_nb_tx_15min_window\"\n",
      "        value {\n",
      "          number_value: 0.0004531264305114746\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_nb_tx_14day_window\"\n",
      "        value {\n",
      "          number_value: 0.00094832181930542\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_avg_amount_7day_window\"\n",
      "        value {\n",
      "          number_value: -0.00160565972328186\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_avg_amount_60min_window\"\n",
      "        value {\n",
      "          number_value: -8.6665153503417969e-05\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_avg_amount_30min_window\"\n",
      "        value {\n",
      "          number_value: -0.0002301990985870361\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_avg_amount_1day_window\"\n",
      "        value {\n",
      "          number_value: 0.0017778038978576659\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_avg_amount_15min_window\"\n",
      "        value {\n",
      "          number_value: -0.0017313420772552489\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"customer_id_avg_amount_14day_window\"\n",
      "        value {\n",
      "          number_value: 0.0025462746620178219\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  output_index: -1\n",
      "  approximation_error: 0.0069232067296640821\n",
      "  output_name: \"probability\"\n",
      "}\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have successfully launched an e2e ML pipeline using Vertex AI Pipeline.\n",
    "\n",
    "Congratuations! You have completed the Vertex AI 101! Before you go, you may want to clean up necessary resources using Notebook: `07_resource_cleanup.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
