{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsv4jGuU89rX"
   },
   "source": [
    "# Master - Environment Setup\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/fraudfinder/raw/main/00_environment_setup.ipynb\">\n",
    "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Google Cloud Notebooks\">Open in Cloud Notebook\n",
    "    </a>\n",
    "  </td> \n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/fraudfinder/blob/main/00_environment_setup.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/fraudfinder/blob/main/00_environment_setup.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "827c41ab1a12"
   },
   "source": [
    "## Overview\n",
    "\n",
    "[FraudFinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the FraudFinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45f6e923dc75"
   },
   "source": [
    "### Objective\n",
    "\n",
    "Before you run this notebook, make sure that you have completed the steps in [README](README.md).\n",
    "\n",
    "In this notebook, you will setup your environment for Fraudfinder to be used in subsequent labs.\n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "- [BigQuery](https://cloud.google.com/bigquery/)\n",
    "- [Google Cloud Storage](https://cloud.google.com/storage)\n",
    "- [Pub/Sub](https://cloud.google.com/pubsub/)\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "- Setup your environment.\n",
    "- Load historical bank transactions into BigQuery.\n",
    "- Read data from BigQuery tables.\n",
    "- Read data from Pub/Sub topics, which contain a live stream of new transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b5e2e2a7bdb"
   },
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04c1dae4ca17"
   },
   "source": [
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* Pub/Sub\n",
    "* BigQuery\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), [Pub/Sub pricing](https://cloud.google.com/pubsub/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "773901ca47fd"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Install the following packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --quiet pip\n",
    "!pip install --upgrade -q -r 'requirements.txt'\n",
    "!pip install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d07214a67580"
   },
   "source": [
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18c113700b6f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f31ae3fed8ab"
   },
   "source": [
    "### Setup your environment\n",
    "\n",
    "Run the next cells to import libraries used in this notebook and configure some options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d61a362443d"
   },
   "source": [
    "Run the next cell to set your project ID and some of the other constants used in the lab.  \n",
    "\n",
    "### Replace REGION Below ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE: REGION will be detected based on the Persistence Resrouces<br>\n",
    "    You will need to run from this cell and below after Kernel restart\n",
    "   </b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Persistent Resource ID\n",
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PERSISTENT_RESOURCE_ID = \"ai-takeoff\"\n",
    "\n",
    "# Dynamically retrieve Persistent Resource location\n",
    "PERSISTENT_RESOURCE_REGION = \"\"\n",
    "check_regions = [\"us-central1\", \"asia-southeast1\", \"europe-west4\"]\n",
    "\n",
    "for region in check_regions:\n",
    "    shell_output = !gcloud ai persistent-resources list --project=$PROJECT_ID --region=$region\n",
    "    if \"Listed 0 items.\" not in shell_output:\n",
    "        print(f\"Persistent Resource found in {region}\")\n",
    "        PERSISTENT_RESOURCE_REGION = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "wxiE6dEWOFm3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Generate unique ID to help w/ unique naming of certain pieces\n",
    "ID = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
    "\n",
    "# Replace Region here\n",
    "REGION = PERSISTENT_RESOURCE_REGION\n",
    "\n",
    "# static parameters\n",
    "\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder-{ID}\"\n",
    "STAGING_BUCKET = f\"{PROJECT_ID}-model-upload-{ID}\"\n",
    "AGENT_BUCKET = f\"{PROJECT_ID}-ai-workshops\"\n",
    "TRAINING_DS_SIZE = 1000\n",
    "\n",
    "with open(\"id_file.txt\", \"w\") as f:\n",
    "    f.write(ID)\n",
    "\n",
    "print(f\"ID '{ID}' has been saved to id_file.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the ID file and set value. This step is just to ensure every participants will have unique buckets and configs across the labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"id_file.txt\", \"r\") as f:\n",
    "        ID = f.read().strip()\n",
    "    print(f\"Using ID '{ID}' from id_file.txt\")\n",
    "except FileNotFoundError:\n",
    "    print(\"id_file.txt not found. Please make sure the file exists.\")\n",
    "    ID = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd738fc1e201"
   },
   "source": [
    "### Create a Google Cloud Storage bucket and save the config data.\n",
    "\n",
    "Next, we will create a Google Cloud Storage bucket and will save the config data in this bucket. After the cell operation finishes, you can navigate to [Google Cloud Storage](https://console.cloud.google.com/storage/) to see the GCS bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d3556c598a6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = f\"\"\"\n",
    "BUCKET_NAME          = \\\"{BUCKET_NAME}\\\"\n",
    "STAGING_BUCKET       = \\\"{STAGING_BUCKET}\\\"\n",
    "PROJECT              = \\\"{PROJECT_ID}\\\"\n",
    "PROJECT_ID           = \\\"{PROJECT_ID}\\\"\n",
    "REGION               = \\\"{REGION}\\\"\n",
    "ID                   = \\\"{ID}\\\"\n",
    "FEATURESTORE_ID      = \\\"fraudfinder_{ID}\\\"\n",
    "MODEL_NAME           = \\\"ff_model\\\"\n",
    "ENDPOINT_NAME        = \\\"ff_model_endpoint\\\"\n",
    "TRAINING_DS_SIZE     = \\\"{TRAINING_DS_SIZE}\\\"\n",
    "DATA_DIR             = \"data\"\n",
    "TRAIN_DATA_DIR       = \"train\"\n",
    "CUSTOMER_ENTITY      = \"customer\"\n",
    "TERMINAL_ENTITY      = \"terminal\"\n",
    "TARGET               = \"tx_fraud\"\n",
    "\"\"\"\n",
    "\n",
    "!gsutil mb -l {REGION} gs://{BUCKET_NAME}\n",
    "!gsutil mb -l {REGION} gs://{STAGING_BUCKET}\n",
    "\n",
    "\n",
    "!echo '{config}' | gsutil cp - gs://{BUCKET_NAME}/config/notebook_env_{ID}.py\n",
    "#!echo '{config}' | gsutil cp - gs://{BUCKET_NAME}/config/notebook_env_v02.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the default BUCKET_URI and SERVICE_ACCOUNT if they were not specified by the user.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin gs://{STAGING_BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = f\"\"\"\n",
    "PROJECT_ID: {PROJECT_ID}\n",
    "STAGING_BUCKET: {STAGING_BUCKET}\n",
    "BUCKET_NAME: {BUCKET_NAME}\n",
    "REGION: {REGION}\n",
    "ID: {ID}\n",
    "CUSTOMER_ENTITY_ID: customer\n",
    "CUSTOMER_ENTITY_ID_FIELD: customer_id\n",
    "TERMINAL_ENTITY_ID: terminal\n",
    "TERMINALS_ENTITY_ID_FIELD: terminal_id\n",
    "FEATURESTORE_ID: \"fraudfinder_{ID}\"\n",
    "FEATUREVIEW_ID: \"fraudfinder_view_{ID}\"\n",
    "NETWORK: fraud-finder-network\n",
    "SUBNET: https://www.googleapis.com/compute/v1/projects/fraud-finder-lab/regions/us-central1/subnetworks/us-central1\n",
    "MODEL_REGISTRY: ff_model\n",
    "RAW_BQ_TRANSACTION_TABLE_URI: \"{PROJECT_ID}.tx.tx\"\n",
    "RAW_BQ_LABELS_TABLE_URI: \"{PROJECT_ID}.tx.txlabels\"\n",
    "FEATURES_BQ_TABLE_URI: \"{PROJECT_ID}.tx.wide_features_table\"\n",
    "FEATURE_TIME: feature_ts\n",
    "ONLINE_STORAGE_NODES: 1\n",
    "SUBSCRIPTION_NAME: ff-tx-for-feat-eng-sub\n",
    "SUBSCRIPTION_PATH: \"projects/{PROJECT_ID}/subscriptions/ff-tx-for-feat-eng-sub\"\n",
    "DROP_COLUMNS:\n",
    "- timestamp\n",
    "- entity_type_customer\n",
    "- entity_type_terminal\n",
    "FEAT_COLUMNS:\n",
    "- customer_id_avg_amount_14day_window\n",
    "- customer_id_avg_amount_15min_window\n",
    "- customer_id_avg_amount_1day_window\n",
    "- customer_id_avg_amount_30min_window\n",
    "- customer_id_avg_amount_60min_window\n",
    "- customer_id_avg_amount_7day_window\n",
    "- customer_id_nb_tx_14day_window\n",
    "- customer_id_nb_tx_15min_window\n",
    "- customer_id_nb_tx_1day_window\n",
    "- customer_id_nb_tx_30min_window\n",
    "- customer_id_nb_tx_60min_window\n",
    "- customer_id_nb_tx_7day_window\n",
    "- terminal_id_avg_amount_15min_window\n",
    "- terminal_id_avg_amount_30min_window\n",
    "- terminal_id_avg_amount_60min_window\n",
    "- terminal_id_nb_tx_14day_window\n",
    "- terminal_id_nb_tx_15min_window\n",
    "- terminal_id_nb_tx_1day_window\n",
    "- terminal_id_nb_tx_30min_window\n",
    "- terminal_id_nb_tx_60min_window\n",
    "- terminal_id_nb_tx_7day_window\n",
    "- terminal_id_risk_14day_window\n",
    "- terminal_id_risk_1day_window\n",
    "- terminal_id_risk_7day_window\n",
    "- tx_amount\n",
    "TARGET_COLUMN: tx_fraud\n",
    "DATA_SCHEMA:\n",
    "  timestamp: object\n",
    "  tx_amount: float64\n",
    "  tx_fraud: Int64\n",
    "  entity_type_customer: Int64\n",
    "  customer_id_nb_tx_1day_window: Int64\n",
    "  customer_id_nb_tx_7day_window: Int64\n",
    "  customer_id_nb_tx_14day_window: Int64\n",
    "  customer_id_avg_amount_1day_window: float64\n",
    "  customer_id_avg_amount_7day_window: float64\n",
    "  customer_id_avg_amount_14day_window: float64\n",
    "  customer_id_nb_tx_15min_window: Int64\n",
    "  customer_id_avg_amount_15min_window: float64\n",
    "  customer_id_nb_tx_30min_window: Int64\n",
    "  customer_id_avg_amount_30min_window: float64\n",
    "  customer_id_nb_tx_60min_window: Int64\n",
    "  customer_id_avg_amount_60min_window: float64\n",
    "  entity_type_terminal: Int64\n",
    "  terminal_id_nb_tx_1day_window: Int64\n",
    "  terminal_id_nb_tx_7day_window: Int64\n",
    "  terminal_id_nb_tx_14day_window: Int64\n",
    "  terminal_id_risk_1day_window: float64\n",
    "  terminal_id_risk_7day_window: float64\n",
    "  terminal_id_risk_14day_window: float64\n",
    "  terminal_id_nb_tx_15min_window: Int64\n",
    "  terminal_id_avg_amount_15min_window: float64\n",
    "  terminal_id_nb_tx_30min_window: Int64\n",
    "  terminal_id_avg_amount_30min_window: float64\n",
    "  terminal_id_nb_tx_60min_window: Int64\n",
    "  terminal_id_avg_amount_60min_window: float64\n",
    "MODEL_NAME: ff_model\n",
    "EXPERIMENT_NAME: ff-experiment-{ID}\n",
    "DATA_URI: gs://{BUCKET_NAME}/data\n",
    "TRAIN_DATA_URI: gs://{BUCKET_NAME}/data/train\n",
    "READ_INSTANCES_TABLE: ground_truth_{ID}\n",
    "READ_INSTANCES_URI: bq://{PROJECT_ID}.tx.ground_truth_{ID}\n",
    "DATASET_NAME: fraud_finder_dataset_{ID}\n",
    "JOB_NAME: fraudfinder-train-xgb-{ID}\n",
    "ENDPOINT_NAME: ff_model_endpoint\n",
    "MODEL_SERVING_IMAGE_URI: \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-7:latest\"\n",
    "IMAGE_REPOSITORY: fraudfinder-{ID}\n",
    "IMAGE_NAME: dask-xgb-classificator\n",
    "IMAGE_TAG: latest\n",
    "IMAGE_URI: {REGION}-docker.pkg.dev/{PROJECT_ID}/fraudfinder-{ID}/dask-xgb-classificator:latest\n",
    "TRAIN_COMPUTE: e2-standard-4\n",
    "DEPLOY_COMPUTE: n1-standard-4\n",
    "BASE_IMAGE: \"python:3.10\"\n",
    "PIPELINE_NAME: \"fraud-finder-xgb-pipeline-{ID}\"\n",
    "PIPELINE_ROOT: \"gs://{BUCKET_NAME}/pipelines\"\n",
    "BQ_DATASET: tx\n",
    "METRICS_URI: \"gs://{BUCKET_NAME}/deliverables/metrics.json\"\n",
    "AVG_PR_THRESHOLD: 0.2\n",
    "MODEL_THRESHOLD: 0.5\n",
    "AVG_PR_CONDITION: avg_pr_condition\n",
    "PERSISTENT_RESOURCE_ID: ai-takeoff\n",
    "REPLICA_COUNT: 1\n",
    "SERVICE_ACCOUNT: \"{SERVICE_ACCOUNT}\"\n",
    "\"\"\"\n",
    "\n",
    "!echo '{config}' | gsutil cp - gs://{BUCKET_NAME}/config/vertex_conf_{ID}.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc2dff7ba2e0"
   },
   "source": [
    "### Copy the historical transaction data into BigQuery tables\n",
    "\n",
    "Now we will copy the historical transaction data and ingest it into BigQuery tables. For this, we will need to run `copy_bigquery_data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder-{ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ac6e0bc33b1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 scripts/user_copy_bigquery_data.py $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29dbf432339c"
   },
   "source": [
    "### Check data in BigQuery\n",
    "\n",
    "After ingesting our data into BigQuery, it's time to run some queries against the tables to inspect the data. You can also go to the [BigQuery console](https://console.cloud.google.com/bigquery) to see the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e12ec3dae852"
   },
   "source": [
    "#### Initialize BigQuery SDK for Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ace8667cc99e"
   },
   "source": [
    "Use a helper function for sending queries to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7afa36c6090",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrapper to use BigQuery client to run query/job, return job ID or result as DF\n",
    "def run_bq_query(sql: str) -> Union[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "    Args:\n",
    "        sql: SQL query, as a string, to execute in BigQuery\n",
    "    Returns:\n",
    "        df: DataFrame of results from query,  or error, if any\n",
    "    \"\"\"\n",
    "\n",
    "    bq_client = bigquery.Client()\n",
    "    \n",
    "\n",
    "    # Try dry run before executing query to catch any errors\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    # If dry run succeeds without errors, proceed to run query\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    job_id = client_result.job_id\n",
    "\n",
    "    # Wait for query/job to finish running. then get & return data frame\n",
    "    df = client_result.result().to_arrow().to_pandas()\n",
    "    print(f\"Finished job_id: {job_id}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20875916c5d4"
   },
   "source": [
    "#### tx.tx\n",
    "The `tx.tx` table contains the basic information about each transaction:\n",
    "- `TX_ID` is a unique ID per transaction\n",
    "- `TX_TS` is the timestamp of the transaction, in UTC\n",
    "- `CUSTOMER_ID` is a unique 16-digit string ID per customer\n",
    "- `TERMINAL_ID` is a unique 16-digit string ID per point-of-sale terminal\n",
    "- `TX_AMOUNT` is the amount of money spent by the customer at a terminal, in dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc0e50b158d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(\n",
    "    \"\"\"\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  tx.tx\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e0ab0d56773"
   },
   "source": [
    "#### tx.txlabels\n",
    "The `tx.txlabels` table contains information on whether each transation was fraud or not:\n",
    "- `TX_ID` is a unique ID per transaction\n",
    "- `TX_FRAUD` is 1 if the transaction was fraud, and 0 if the transaction was not fraudulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c128a6c78e82",
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bq_query(\n",
    "    \"\"\"\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  tx.txlabels\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffdfcfed70bd"
   },
   "source": [
    "### Check live streaming transactions via public Pub/Sub topics\n",
    "\n",
    "As part of the [README](README.md), you've created [subscriptions](https://console.cloud.google.com/cloudpubsub/subscription/) to public Pub/Sub topics, where there is a constant flow of new transactions. This means you have, in your own Google Cloud project, subscriptions to the public Pub/Sub topics. You will receive a Pub/Sub message in your subscription every time a new transaction is streamed into the Pub/Sub topic.\n",
    "\n",
    "There are two public Pub/Sub topics where there is a constant stream of live transactions occurring.\n",
    "\n",
    "The following Pub/Sub topics are used for transactions:\n",
    "```\n",
    "projects/cymbal-fraudfinder/topics/ff-tx\n",
    "projects/cymbal-fraudfinder/topics/ff-txlabels\n",
    "```\n",
    "\n",
    "Note: If you haven't completed the steps in the README, please make sure that you complete them first before continuing this notebook, otherwise you may not have Pub/Sub subscriptions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "00_environment_setup.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
